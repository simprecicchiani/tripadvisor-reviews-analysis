{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copia di 02a - A real-world problem\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1KkUgyHuZMPpV-t28FoYXzaxfhRf48Iu8\n",
        "\n",
        "**Copyright 2021 Sapienza NLP research group (http://nlp.uniroma1.it/)**\n",
        "\n",
        "Authors: Edoardo Barba, Michele Bevilacqua, Simone Conia, Luigi Procopio, Roberto Navigli\n",
        "\n",
        "All the material is made available under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 License (international): https://creativecommons.org/licenses/by-nc-sa/4.0/.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Hello and welcome to our Practical Deep Learning for NLP 101 course! In this Chapter, we'll get our hands dirty with a more realistic problem. We all love the XOR function, but why don't we review videogames instead?\n",
        "\n",
        "Let's read the reviews that buyers wrote for some videogames and see if we can guess their opinions!\n",
        "\n",
        "# Preparing the Environment\n",
        "\n",
        "Differently from the previous Chapter, this time we will be working with a real-world dataset. First thing first, we'll need to retrieve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sample_data: we don't need this folder\n",
        "! rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's create a new folder and download the dataset. We'll also need to unzip it\n",
        "! mkdir data\n",
        "! wget -O data/amazon-reviews.tsv.gz https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_Games_v1_00.tsv.gz\n",
        "! gzip -d data/amazon-reviews.tsv.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reminder:** this is bash! Learning to properly alternate between python, bash and other \"languages\" is something vital in data science. Surely python is nice, but isn't it overkill to use it to download a file or create a folder?\n",
        "\n",
        "Let's import a few packages that we'll need through the notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# general\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import *\n",
        "# %%\n",
        "# torch\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon Reviews\n",
        "\n",
        "The dataset we are going to use is [Amazon Reviews](https://s3.amazonaws.com/amazon-reviews-pds/readme.html); it is a well-known dataset in the ML community that is frequently used in multiple settings.\n",
        "\n",
        "As it's really gigantic, it is divided in different sub-datasets based on the Amazon categories we all know. Today, we'll use the VideoGames sub-dataset.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "So, first of all, let's check its format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! head -5 data/amazon-reviews.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a TSV file. TSV is a format you'll soon get very acquainted with; while now it may appear pretty much unreadable, you'll be preying to encounter it when searching for datasets (way worse and unfathomable formats exist).\n",
        "\n",
        "Indeed, it's extremely easy to parse. Each line represents a data sample and information for this sample are reported into tab-separated columns. Often (i.e. when you are lucky as in this case), the first line is used to report what each column stands for.\n",
        "\n",
        "Here, we are interested in **reviews** (column 14) and their corresponding **number of stars** (column 8); thus, columns 8 and 14 have all the information we need.\n",
        "\n",
        "Let's discard all the other columns and make the file more readable; we will stick with bash so that you get familiar with its usage (why don't you try to do the same with python? Meanwhile, if you don't know the meaning of *tail*, *cut* and *head*, check them out)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! tail +2 data/amazon-reviews.tsv | cut -f8,14 > data/dataset.raw.tsv \n",
        "! head -5 data/dataset.raw.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, it's something humans can read! We can now formulate the goal of this Chapter more explicitly: given a review, we want to learn a function $f$ that predicts how many stars the reviewer originally gave.\n",
        "\n",
        "Thus, if we receive *poor quality work and not as it is advertised*, we want to predict $1$. Conversely, *Loved it,  I didn't even realise it was a gaming mouse...* is clearly a $5$!\n",
        "\n",
        "**!!! TIP !!!**\n",
        "\n",
        "Every time you get your hands on new data, before doing anything else, it's crucial that you investigate them a bit. You can extract useful insights from them that might help you later on.\n",
        "\n",
        "In this case, there's a natural question that arises. How are the stars distributed? Do we have as many $1$-s as $5$-s? Are $5$-s the most frequent scores? Or are they $3$-s? Let's check it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR/0lEQVR4nO3df7BcZ13H8ffHtBWhQNFcmdIEUjQFIiMKoaiAFEFIKFidUWxRarHYKVIEQSQqIIKjIKPDMBRirLH80FYQ1FgCRYVaESpNoS1NS5lMKO0lhdxSKLSAJeXrH3uiy3Zzd2+6927y3PdrZid7znn2eb6nTT732WfP2ZuqQpJ0+PueaRcgSZoMA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGupqVpJL8cPd8c5JXTajfBye5PcmKbvuSJM+fRN9dfx9I8muT6k/Lh4GuRZHkCUk+luS2JLcm+a8kj+2OnZHko0tZT1WdXVWvG9UuyQ1Jnjqirxur6uiquuue1pXkNUneNdD/xqp6+z3tW8vPEdMuQO1Jcj/gIuAFwLuBo4AnAv8zof6PqKp9k+jrcBpbGsUZuhbDCQBVdUFV3VVV36yqD1XV1UkeAWwGfrJbtvgqQJKTk3wqydeS3JTkNfs7S7KmWz45M8mNwIeHDZrk5UluTrInya8PHDs/yR93z1cmuSjJV7t3D/+Z5HuSvBN4MPAvXW2/O2zsvn39E6IfSvKJ7h3JPyf5/m6sk5LMDtRyQ5KnJtkA/D7wy914V3XH/28Jp6vrlUk+n2Rvknckuf/Af5dfS3JjkluS/MFB/R9TEwx0LYbPAncleXuSjUkesP9AVV0HnA18vFu2OKY7dAdwOnAMcDLwgiQ/P9Dvk4BHAE8fHLALx98BfhZYC8y3bPIyYBaYAR5IL1Srqp4L3Ag8q6vtz8YZu3M68OvAg4B9wJvnGR96A34Q+BPg77vxHjWk2Rnd48nAQ4GjgbcMtHkC8DDgKcCrux+aWoamGuhJtnazjmvGbP/sJNcm2Znk7xa7Ph2cqvoavZAp4K+AuSTbkjxwntdcUlWfrqrvVNXVwAX0QrTfa6rqjqr65pAung38TVVdU1V3AK+Zp8RvA8cCD6mqb1fVf9boLzWab2yAd/aN/Srg2fs/NL2HfgX4i6raXVW3A78HnDrw7uCPundBVwFXAcN+MGgZmPYM/XxgwzgNk6yl95f58VX1I8BLFq8s3VNVdV1VnVFVq4BH0pu5vulA7ZM8LslHkswluY3eLH7lQLOb5hnyQQPHPz9P2zcCu4APJdmdZNM8bccZe/D454EjuXv9B+NBfPe5fJ7eZ1/9Pxy/2Pf8G/Rm8VqGphroVXUpcGv/viQ/lOSDSa7o1jYf3h36DeDcqvpK99q9S1yuDlJVfYbeD+9H7t81pNnfAduA1VV1f3rr7Bnsap5hbgZW920/eJ56vl5VL6uqhwLPAl6a5Ckjxhg1gx8c+9vALfSWku69/0A3a59ZQL97gIcM9L0P+NKI12kZmvYMfZgtwIuq6jH01kTf2u0/ATihu/ztsm7NVIegJA9P8rIkq7rt1cBpwGVdky8Bq5Ic1fey+wK3VtW3kpwIPGeBw74bOCPJuiT3Bv5wnvqemeSHkwT4GnBX99hf20MXODbAr/aN/VrgH7rLGj8L3Kv70PdI4JXA9/a97kvAmiQH+rd4AfDbSY5PcjT/v+bulTa6m0Mq0Lu/sD8FvCfJlcBf0lvrhN7bzLXASfTC4bwkxyx9lRrD14HHAf+d5A56QX4NvQ8joXeVyk7gi0lu6fb9JvDaJF8HXk0voMdWVR+gt6TzYXrLKUOvhOmsBf4NuB34OPDWqrqkO/anwCu7K2B+ZwElvJPeu5AvAvcCfqur6zZ653Ye8AV6M/b+q17e0/355SSfHNLv1q7vS4HPAd8CXrSAurSMZNq/4CLJGuCiqnpketcvX19Vxw5ptxm4rKrO77b/HdhUVZcvZb2SdKg6pGbo3dURn0vySwDp2f+J/T/Ru3SLJCvpLcHsnkadknQomvZlixfQe8v7sCSzSc6kd5nWmd1NFjuBU7rmF9N7W3ot8BHg5VX15WnULUmHoqkvuUiSJuOQWnKRJB28qX0518qVK2vNmjXTGl6SDktXXHHFLVU1M+zY1AJ9zZo17NixY1rDS9JhKckB74J2yUWSGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhoxtTtFJWkh1mx6/7RLmJgbXn/yovTrDF2SGjEy0JNsTbI3yTUHOJ4kb06yK8nVSR49+TIlSaOMM0M/H5jvFzJvpPc7GtcCZwFvu+dlSZIWamSgV9WlwK3zNDkFeEf1XAYck+RuvxNUkrS4JrGGfhxwU9/2bLfvbpKclWRHkh1zc3MTGFqStN8kAj1D9g39vXZVtaWq1lfV+pmZod/PLkk6SJMI9Flgdd/2KmDPBPqVJC3AJAJ9G3B6d7XLTwC3VdXNE+hXkrQAI28sSnIBcBKwMsks8IfAkQBVtRnYDjwD2AV8A3jeYhUrSTqwkYFeVaeNOF7ACydWkSTpoHinqCQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRgr0JNsSHJ9kl1JNg05fv8k/5LkqiQ7kzxv8qVKkuYzMtCTrADOBTYC64DTkqwbaPZC4NqqehRwEvDnSY6acK2SpHmMM0M/EdhVVbur6k7gQuCUgTYF3DdJgKOBW4F9E61UkjSvcQL9OOCmvu3Zbl+/twCPAPYAnwZeXFXfGewoyVlJdiTZMTc3d5AlS5KGGSfQM2RfDWw/HbgSeBDwY8Bbktzvbi+q2lJV66tq/czMzAJLlSTNZ5xAnwVW922vojcT7/c84H3Vswv4HPDwyZQoSRrHOIF+ObA2yfHdB52nAtsG2twIPAUgyQOBhwG7J1moJGl+R4xqUFX7kpwDXAysALZW1c4kZ3fHNwOvA85P8ml6SzSvqKpbFrFuSdKAkYEOUFXbge0D+zb3Pd8DPG2ypUmSFsI7RSWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNGCvQk2xIcn2SXUk2HaDNSUmuTLIzyX9MtkxJ0ihHjGqQZAVwLvCzwCxweZJtVXVtX5tjgLcCG6rqxiQ/uEj1SpIOYJwZ+onArqraXVV3AhcCpwy0eQ7wvqq6EaCq9k62TEnSKOME+nHATX3bs92+ficAD0hySZIrkpw+rKMkZyXZkWTH3NzcwVUsSRpqnEDPkH01sH0E8BjgZODpwKuSnHC3F1Vtqar1VbV+ZmZmwcVKkg5s5Bo6vRn56r7tVcCeIW1uqao7gDuSXAo8CvjsRKqUJI00zgz9cmBtkuOTHAWcCmwbaPPPwBOTHJHk3sDjgOsmW6okaT4jZ+hVtS/JOcDFwApga1XtTHJ2d3xzVV2X5IPA1cB3gPOq6prFLFyS9N3GWXKhqrYD2wf2bR7YfiPwxsmVJklaCO8UlaRGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEWMFepINSa5PsivJpnnaPTbJXUl+cXIlSpLGMTLQk6wAzgU2AuuA05KsO0C7NwAXT7pISdJo48zQTwR2VdXuqroTuBA4ZUi7FwHvBfZOsD5J0pjGCfTjgJv6tme7ff8nyXHALwCb5+soyVlJdiTZMTc3t9BaJUnzGCfQM2RfDWy/CXhFVd01X0dVtaWq1lfV+pmZmTFLlCSN44gx2swCq/u2VwF7BtqsBy5MArASeEaSfVX1T5MoUpI02jiBfjmwNsnxwBeAU4Hn9DeoquP3P09yPnCRYS5JS2tkoFfVviTn0Lt6ZQWwtap2Jjm7Oz7vurkkaWmMM0OnqrYD2wf2DQ3yqjrjnpclSVoo7xSVpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMdZ3uUg6NKzZ9P5plzARN7z+5GmX0CRn6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1Ijxgr0JBuSXJ9kV5JNQ47/SpKru8fHkjxq8qVKkuYzMtCTrADOBTYC64DTkqwbaPY54ElV9aPA64Atky5UkjS/cWboJwK7qmp3Vd0JXAic0t+gqj5WVV/pNi8DVk22TEnSKOME+nHATX3bs92+AzkT+MCwA0nOSrIjyY65ubnxq5QkjTROoGfIvhraMHkyvUB/xbDjVbWlqtZX1fqZmZnxq5QkjXTEGG1mgdV926uAPYONkvwocB6wsaq+PJnyhluz6f2L2f2SuuH1J0+7BEmNGGeGfjmwNsnxSY4CTgW29TdI8mDgfcBzq+qzky9TkjTKyBl6Ve1Lcg5wMbAC2FpVO5Oc3R3fDLwa+AHgrUkA9lXV+sUrW5I0aJwlF6pqO7B9YN/mvufPB54/2dIkSQvhnaKS1AgDXZIaYaBLUiMMdElqxFgfikqHCu9BkA7MGbokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIv8vlMOT3mUgaxhm6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjRgr0JNsSHJ9kl1JNg05niRv7o5fneTRky9VkjSfkYGeZAVwLrARWAeclmTdQLONwNrucRbwtgnXKUkaYZwZ+onArqraXVV3AhcCpwy0OQV4R/VcBhyT5NgJ1ypJmsc4v4LuOOCmvu1Z4HFjtDkOuLm/UZKz6M3gAW5Pcv2Cql16K4FbFnOAvGExe79HFv3cYXmfv+d+SDoc/t4/5EAHxgn0DNlXB9GGqtoCbBljzENCkh1VtX7adUzDcj53WN7n77kfvuc+zpLLLLC6b3sVsOcg2kiSFtE4gX45sDbJ8UmOAk4Ftg202Qac3l3t8hPAbVV182BHkqTFM3LJpar2JTkHuBhYAWytqp1Jzu6Obwa2A88AdgHfAJ63eCUvqcNmeWgRLOdzh+V9/p77YSpVd1vqliQdhrxTVJIaYaBLUiMM9CGSbE2yN8k1065lqSVZneQjSa5LsjPJi6dd01JJcq8kn0hyVXfufzTtmpZakhVJPpXkomnXstSS3JDk00muTLJj2vUcDNfQh0jy08Dt9O5+feS061lK3R2+x1bVJ5PcF7gC+PmqunbKpS26JAHuU1W3JzkS+Cjw4u7u52UhyUuB9cD9quqZ065nKSW5AVhfVYt+Y9FicYY+RFVdCtw67TqmoapurqpPds+/DlxH767f5nVfXXF7t3lk91g2M54kq4CTgfOmXYsOjoGuA0qyBvhx4L+nXMqS6ZYcrgT2Av9aVcvm3IE3Ab8LfGfKdUxLAR9KckX3NSWHHQNdQyU5Gngv8JKq+tq061kqVXVXVf0YvbudT0yyLJbckjwT2FtVV0y7lil6fFU9mt63x76wW3o9rBjouptu/fi9wN9W1fumXc80VNVXgUuADdOtZMk8Hvi5bh35QuBnkrxruiUtrara0/25F/hHet80e1gx0PVdug8G/xq4rqr+Ytr1LKUkM0mO6Z5/H/BU4DNTLWqJVNXvVdWqqlpD7+s9PlxVvzrlspZMkvt0FwGQ5D7A04DD7io3A32IJBcAHwcelmQ2yZnTrmkJPR54Lr0Z2pXd4xnTLmqJHAt8JMnV9L7D6F+ratldvrdMPRD4aJKrgE8A76+qD065pgXzskVJaoQzdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGvG/F8jRZpsJ7uQAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "def visualize_n_stars(dataset: str, title: str):\n",
        "    \n",
        "    star_counter = Counter()\n",
        "\n",
        "    with open(dataset) as f:\n",
        "        for line in f:\n",
        "            star = int(line.strip().split('\\t')[0])\n",
        "            star_counter[star] += 1\n",
        "\n",
        "    stars = sorted(star_counter)\n",
        "    values = [star_counter[star] for star in stars]\n",
        "\n",
        "    plt.bar(stars, values)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "visualize_n_stars('data/dataset.raw.tsv', 'Star distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, $5$-s are by far the most frequent scores!\n",
        "\n",
        "**Exercise.** Compute statistics for the words in the reviews and see which words are most frequent.\n",
        "\n",
        "## Simplification\n",
        "\n",
        "As a matter of fact, this task is **extremely complex**, especially as we are restrained with the few tools presented so far. There are $2$ major obstacles:\n",
        "* This task is way more complex than a simple XOR function. Even if we did this task manually ourselves, we still woudn't achieve 100% accuracy!\n",
        "* How do we deal with text?\n",
        "\n",
        "In order to deal with both, we will simplify the task. In particular, starting from the official dataset, let's create a new one that is easier.\n",
        "\n",
        "How? Simple! Let's define the following set of markers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_markers = {'fantastic', 'amazing', 'excellent', 'very good'}\n",
        "neutral_markers = {'adequate', 'fine but', 'good but', 'ok but'}\n",
        "negative_markers = {'returning', 'sucks', 'waste'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will discard all positive reviews ($4$ and $5$) that do not contain the above positive markers; we'll repeat the same for neutral and negative reviews.\n",
        "\n",
        "**More complex:** try to add some confusing markers to make the dataset harder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3844ae8bef774d08b0900210d633344c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "markers = positive_markers | neutral_markers | negative_markers\n",
        "assert len(markers) == len(positive_markers) + len(neutral_markers) + len(negative_markers)\n",
        "\n",
        "star2markers = {\n",
        "    1: negative_markers,\n",
        "    2: negative_markers,\n",
        "    3: neutral_markers,\n",
        "    4: positive_markers,\n",
        "    5: positive_markers\n",
        "}\n",
        "\n",
        "progress_bar = tqdm()\n",
        "star_writes = Counter()\n",
        "\n",
        "with open('data/dataset.raw.tsv') as fi, open('data/dataset.tsv', 'w') as fo:\n",
        "\n",
        "    for i, line in enumerate(fi):\n",
        "\n",
        "        try:\n",
        "\n",
        "            star, review = line.strip().lower().split('\\t')\n",
        "            star = int(star)\n",
        "\n",
        "            assert len(review) > 20 and len(review) < 100\n",
        "            assert any(m in review for m in star2markers[star])\n",
        "\n",
        "            star_writes[star] += 1\n",
        "            fo.write(f'{star}\\t{review}\\n')\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        progress_bar.update()\n",
        "        if i % 1_000 == 0:\n",
        "            # ** uses the dictionary as individual parameters with the key name and their value\n",
        "            progress_bar.set_postfix(**{str(k): v for k, v in star_writes.items()})\n",
        "\n",
        "progress_bar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have now generated a dataset that:\n",
        "* is way smaller than the original one\n",
        "* has **clear patterns** that denote the expected number of stars\n",
        "\n",
        "Finally, let's split it into $2$ parts:\n",
        "* *train*, a large chunk of data that we'll use to train our NN\n",
        "* *test*, a smaller chunk that we'll use to report the results\n",
        "\n",
        "We'll talk more about this splitting strategy in the next Chapter. For the moment, suffice it to say that we can use *test* as a proxy of the generalization capability of our network since our NN is not trained upon its data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    1000 data/test.tsv\n",
            "   17315 data/train.tsv\n",
            "   18315 total\n"
          ]
        }
      ],
      "source": [
        "! head -1000 data/dataset.tsv > data/test.tsv\n",
        "! tail +1001 data/dataset.tsv > data/train.tsv\n",
        "! wc -l data/test.tsv data/train.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Always check the stats!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "visualize_n_stars('data/train.tsv', 'train')\n",
        "visualize_n_stars('data/test.tsv', 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Extraction\n",
        "\n",
        "So, how do we deal with text? As we have seen in the previous Chapter, a neural network is composed of matrices and its input and output are expected to be matrices as well.\n",
        "\n",
        "Thus, we need to somehow convert reviews into matrices. This process is called **feature extraction**. Let's start with something simple to get a general idea.\n",
        "\n",
        "Imagine we have two reviews:\n",
        "* *this game is a waste of money*, whose rating was a $1$\n",
        "* *truly amazing game!*, whose rating was a $5$\n",
        "\n",
        "Isn't it true that, if a review contains the word *waste*, then it's a $1$? And, similarly, if it contains the word *amazing*, then it's a $5$? Yes! Indeed, *amazing* and *waste* are effective markers that can be used to detect the orientation of a given review. This is exactly what a feature is!\n",
        "\n",
        "Let's exploit this to define a function $\\phi: r \\rightarrow \\mathbb{R}^{2}$, that is a function that takes as input a review $r$ and emits a 2d vector $x$, constructed as follows:\n",
        "* $x_0 = 1$ if *amazing* is present in $r$; else $x_0 = 0$\n",
        "* $x_1 = 1$ if *waste* is present in $r$; else $x_1 = 0$\n",
        "\n",
        "Thus:\n",
        "\n",
        "\\\\[ \\phi(\\text{this game is a waste of money}) = \\begin{bmatrix} 0.0, 1.0 \\end{bmatrix} \\\\]\n",
        "\\\\[ \\phi(\\text{truly amazing game}) = \\begin{bmatrix} 1.0, 0.0 \\end{bmatrix} \\\\]\n",
        "\n",
        "Leveraging $\\phi$, approximating our target function $f$ with a NN is now something we can do!\n",
        "\n",
        "**!!! NOTE !!!**\n",
        "\n",
        "Of course, there are obvious limitations to this approach; just imagine *I was hoping for an amazing game, instead this is terrible!*, which is clearly a $1$. \n",
        "Yet, regardless of its simplicity, the crucial point of this section is that you understand function $\\phi$ and its role in NN-s; once you grasp it, scaling to more heterogenous and realistic scenarios will be a matter of swapping the function $\\phi$. Bear with us!\n",
        "\n",
        "We have seen how we can build a function $\\phi$ from a list of markers; however, a further question naturally arises: what markers should we use?\n",
        "\n",
        "As it turns out, we know all the possible markers we are interested in! Indeed, with this very objective in mind, we built the simplified dataset by specifying a list of markers that had to be present in positive, neutral and negative reviews!\n",
        "\n",
        "Let's implement our real function $\\phi$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
            "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.])\n"
          ]
        }
      ],
      "source": [
        "marker2idx = {marker: idx for idx, marker in enumerate(markers)}\n",
        "\n",
        "def review2vector(review: str):\n",
        "    vector = torch.zeros(len(marker2idx), dtype=torch.float)\n",
        "    for marker, idx in marker2idx.items():\n",
        "        if marker in review:\n",
        "            vector[idx] = 1\n",
        "    return vector\n",
        "\n",
        "# 4 star review\n",
        "print(review2vector('very good product. excellent quality'))\n",
        "\n",
        "# 1 star review\n",
        "print(review2vector('It echoes so badly with my voice. Don\\'t waste your time. I\\'m heavily considering returning it.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, we can now convert any review $r$ into a vector $v$ that can be fed to a neural network!\n",
        "\n",
        "# Training Procedure\n",
        "\n",
        "Now that we have both data and $\\phi$, let's get back to neural networks. We'll first zoom on a couple of details we glimpsed over in the previous chapter and then recast our problem in $3$ different ways, each using a different type of loss (and slightly different architecture).\n",
        "\n",
        "## More on PyTorch: Datasets and DataLoaders\n",
        "\n",
        "In the previous Chapter, we mentioned how neural networks' training is commonly performed over batches, rather than samples; this is motivated by both mathematical and efficiency reasons. However, we wanted to keep things simple and did not present how this is actually done in PyTorch. As it turns out, it's actually very easy.\n",
        "\n",
        "Virtually the entirety of PyTorch data handling builds upon $2$ concepts:\n",
        "* *Dataset*\n",
        "* *DataLoader*\n",
        "\n",
        "A *Dataset* is a simple abstraction that represents a collection of samples which can be emitted in tensor form. In its simplest version (*map-style*), it's a class that implements the \\_\\_getitem\\_\\_() and \\_\\_len\\_\\_() methods; in our case; a Dataset is something we can access with the $dataset[idx]$ syntax, which will yield the $idx$-th review, along with the associated number of stars, both of them in tensor form.\n",
        "\n",
        "Let's implement the PyTorch Dataset for our task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReviewDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path: str, feature_extraction_function: Callable[[str], torch.tensor]):\n",
        "        # standard constructor\n",
        "        self.dataset_path = dataset_path\n",
        "        self.feature_extraction_function = feature_extraction_function\n",
        "        # call to init the data\n",
        "        self._init_data()\n",
        "\n",
        "    def _init_data(self):\n",
        "        # iterate on the given file and build samples\n",
        "        self.samples = []\n",
        "        with open(self.dataset_path) as f:\n",
        "            for line in f:\n",
        "                star, review = line.strip().split('\\t')\n",
        "                # use the feature extraction function to convert reviews into tensors\n",
        "                self.samples.append((self.feature_extraction_function(review), torch.tensor(int(star))))\n",
        "                \n",
        "    def __len__(self):\n",
        "        # returns the number of samples in our dataset\n",
        "      return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns the idx-th sample\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ok, now that we defined the class, we can actually read the *train* and *test* dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]), tensor(5))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset = ReviewDataset('data/train.tsv', review2vector)\n",
        "test_dataset = ReviewDataset('data/test.tsv', review2vector)\n",
        "\n",
        "# what's inside a sample?\n",
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the other hand, a *DataLoader* is the component that bridges the gap between samples and batches. We will see that, as the problems and the approximating $f$-s we consider become more convoluted, a *DataLoader* is a powerful component that takes care of quite a few nuisances.\n",
        "\n",
        "It is quite easy to build DataLoader-s from Dataset-s. We'll use $32$ as the batch size, meaning the batches will be composed of $32$ samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# batch len: 32\n",
            "# batch x: \n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]])\n",
            "# batch y: \n",
            "tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 1, 5, 5, 3, 5, 5, 5, 5, 1, 5, 5, 5,\n",
            "        5, 5, 1, 5, 5, 5, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    # the type of batch is: Tuple[torch.Tensor, torch.Tensor]\n",
        "    batch_x, batch_y = batch\n",
        "    print(f'# batch len: {len(batch_x)}')\n",
        "    print(f'# batch x: \\n{batch_x}')\n",
        "    print(f'# batch y: \\n{batch_y}')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, while ReviewDataset reads a list of pairs $(x, y)$, with $x$ being the review and $y$ the score, we can use the dataloader to seamlessly batch them.\n",
        "\n",
        "## 3 Losses\n",
        "\n",
        "Up to this point, we have only shown the MSE loss. This loss is perfect in regression problems (i.e. $f(x)=2x$). However, when dealing with **classification tasks**, such as determining whether a given image contains a dog, a cat or a bird, MSE is not appropriate for a number of mathematical reasons we won't delve in; rather, losses such as the cross-entropy loss are used.\n",
        "\n",
        "Our star-prediction problem can be treated both as a regression and classification task; indeed, we can frame it either as the prediction of a *float* in the $[1, 5]$ range, whose output we round at postprocessing, or as the selection of the best class $c \\in \\{1, 2, 3, 4, 5\\}$ for the considered review.\n",
        "\n",
        " We'll first consider the former configuration, as we are already familiar with it and its MSE loss. Then, we'll consider $2$ different classification variants.\n",
        "\n",
        "Let's first define a training loop function so that we can avoid to re-implementing it every time. We will also pass an *adapt* hook that takes as input the batched $x$ and $y$ and adapts to them the variant being considered; it'll become clear later the purpose of this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_loop(model: nn.Module, optimizer: torch.optim.Optimizer,  adapt: Callable[[torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor]], epochs: int = 5):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        progress_bar = tqdm()\n",
        "\n",
        "        # batches of the training set\n",
        "        for x, y in train_dataloader:\n",
        "\n",
        "            x, y = adapt(x, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_out = model(x, y)\n",
        "            loss = batch_out['loss']\n",
        "\n",
        "            # computes the gradient of the loss\n",
        "            loss.backward()\n",
        "            # updates parameters based on the gradient information\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.update()\n",
        "            progress_bar.set_postfix(epoch=epoch, loss=loss.item())\n",
        "        \n",
        "        progress_bar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predict the Number of Stars: MSE\n",
        "\n",
        "**Problem**: given a review $r$, emit a real number in $[1, 5]$ that denotes the number of stars the user originally gave to $r$. As this number is an integer, we'll have to round the output of the NN.\n",
        "\n",
        "Let's first define our NN module. There is nothing fancy about it, it's a simple 2-layered feed-forward network with a RELU activation in the middle.\n",
        "\n",
        "The only noteworthy point is the fact that the forward method returns a dictionary. This is quite common in several NN frameworks as it allows to return multiple values with an easily-accessible syntax (like *loss* and *pred* in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c2ef7a401394dfeaec6c84433a0ab69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b9f01740fef423ab96a222245fe3ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a715581721643acb20597d016345658",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66e799be82914be7a09a4a206875426e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34e6eeba3059419dac5e43520031f352",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_features: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(n_features, n_hidden)\n",
        "        self.lin2 = torch.nn.Linear(n_hidden, 1)\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "        # actual forward\n",
        "        out = self.lin1(x)\n",
        "        out = torch.relu(out)\n",
        "        out = self.lin2(out).squeeze(1)\n",
        "\n",
        "        result = {'pred': out}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            loss = self.loss(out, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)\n",
        "\n",
        "model = Classifier(len(marker2idx), 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n",
        "\n",
        "training_loop(model, optimizer, adapt=lambda x, y : (x, y.float()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check a couple of predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============================================================================================================================\n",
            "# review: very good product. excellent quality\n",
            "# gold stars: 4\n",
            "# predicted score: 5.123114585876465 -> 5\n",
            "=============================================================================================================================\n",
            "=============================================================================================================================\n",
            "# review: It echoes so badly with my voice. Don't waste your time. I'm heavily considering returning it.\n",
            "# gold stars: 1\n",
            "# predicted score: 0.7180482149124146 -> 1\n",
            "=============================================================================================================================\n",
            "# accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "def predict(review: str):\n",
        "\n",
        "    # we need to vectorize the review\n",
        "    x = review2vector(review)\n",
        "\n",
        "    # our nn expects a batched input\n",
        "    # even if we have a single sample, we have to mock this structure converting a (11)-shape vector into (1, 11)-shape one\n",
        "    batched_x = x.unsqueeze(0)\n",
        "\n",
        "    # actual forward\n",
        "    batch_out = model(batched_x)\n",
        "\n",
        "    # return\n",
        "    return batch_out['pred'][0].item()\n",
        "\n",
        "def predict_and_print(review: str, stars: int):\n",
        "    print(f'=' * 125)\n",
        "    print(f'# review: {review}')\n",
        "    print(f'# gold stars: {stars}')\n",
        "    score = predict(review)\n",
        "    print(f'# predicted score: {score} -> {round(score)}')\n",
        "    print(f'=' * 125)\n",
        "\n",
        "\n",
        "# 4 star review\n",
        "predict_and_print('very good product. excellent quality', 4)\n",
        "\n",
        "# 1 star review\n",
        "predict_and_print('It echoes so badly with my voice. Don\\'t waste your time. I\\'m heavily considering returning it.', 1)\n",
        "\n",
        "\"\"\"Ok so it seems the network should be working. Let's evaluate its performance on the test set.\"\"\"\n",
        "\n",
        "n = 0\n",
        "d = 0\n",
        "\n",
        "# for each batch in the test set\n",
        "for x, y in test_dataloader:\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # classify the batch\n",
        "            batch_out = model(x)\n",
        "            pred = batch_out['pred']\n",
        "\n",
        "        pred = torch.round(pred)\n",
        "        # number of predictions (corresponding to number of batch items to predict)\n",
        "        d += pred.shape[0]\n",
        "        # number of correct classifications within the batch\n",
        "        n += (y == pred).int().sum()\n",
        "\n",
        "print(f'# accuracy: {(n / d).item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not bad!\n",
        "\n",
        "# Binary Cross-Entropy\n",
        "\n",
        "**Problem**: given a review $r$, classify whether it is positive or negative. We are no longer interested in the number of stars, we just want to know if it is positive or negative.\n",
        "\n",
        "To convert our dataset into this format, we'll say that any review with a number of stars $> 2.5$ is positive, otherwise it is negative.\n",
        "\n",
        "This is a **classification problem**. Given a review $r$, we want to determine the value $y \\in \\{0, 1\\}$, with $0$ denoting a negative review and $1$ a positive one.\n",
        "\n",
        "As we were mentioning before, MSE is not adequate for a number of mathematical reasons (try searching them!). Rather, for binary classification, binary cross-entropy is the loss most commonly used:\n",
        "\n",
        "\\\\[ \\mathcal{L}_{BCE} = - \\frac{1}{|y|}\\sum_{i=1}^{|y|} y_i log(p(y_i)) + (1 - y_i) log (1 - p(y_i)) \\\\]\n",
        "\n",
        "While the theory behind this formula goes beyond the scope of this course, the general intuition is quite easy: it will return high values for bad predictions and low values for good ones. Casting it to our reviews' scenario, if $y=1$, BCE will encourage a high $p(y_i = 1)$ value and low $p(y_i = 0)$ one; conversely, if $y=0$, BCE will encourage the opposite behavior.\n",
        "\n",
        "However, BCE requires one significant change to our architecture. The above formula requires our NN to **emit a probability**; that is, the final layer of the NN must emit a probability $p(y_i) \\in [0, 1]$. Yet, currently the final layer emits a number in $[-\\infty, \\infty]$; how can we transform it into a probability?\n",
        "\n",
        "Enter the **sigmoid** function!\n",
        "\n",
        "\\\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\\\]\n",
        "\n",
        "Plotting it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "k = 15\n",
        "\n",
        "x = np.array(list(range(-k, k + 1)))\n",
        "y = 1 / (1 + np.e ** (-x))\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.xlim(-k, k)\n",
        "plt.ylim(0, 1)\n",
        "plt.title('Sigmoid')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That is, sigmoid is an activation function that can map any real number into the $[0, 1]$ range, thus effectively converting it into a probability.\n",
        "\n",
        "In order to apply BCE, we need to apply a sigmoid function to the output of the last layer. The resulting value, now a probability, will be the input to our loss function.\n",
        "\n",
        "Let's get back to coding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "619d634789ce48b6a8cb2a3d0ccfab15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5cde7d2a83543fd87e1228e9cc4d1ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bd8f4673dcb423e871d8dbd699420a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b5588dfe33446cb2ab5e91d2ff3cce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01358c963e8b4f898a3cd1f6b349e5e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============================================================================================================================\n",
            "# review: very good product. excellent quality\n",
            "# gold stars: 4 -> 1\n",
            "# predicted score: 0.9999984502792358 -> 1\n",
            "=============================================================================================================================\n",
            "=============================================================================================================================\n",
            "# review: It echoes so badly with my voice. Don't waste your time. I'm heavily considering returning it.\n",
            "# gold stars: 1 -> 0\n",
            "# predicted score: 0.0017232667887583375 -> 0\n",
            "=============================================================================================================================\n",
            "# accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_features: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(n_features, n_hidden)\n",
        "        self.lin2 = torch.nn.Linear(n_hidden, 1)\n",
        "        # the loss_fn has now become the BCELoss\n",
        "        self.loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "        # actual forward\n",
        "        out = self.lin1(x)\n",
        "        out = torch.relu(out)\n",
        "        out = self.lin2(out).squeeze(1)\n",
        "        # we need to apply a sigmoid activation function\n",
        "        out = torch.sigmoid(out)\n",
        "\n",
        "        result = {'pred': out}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            loss = self.loss(out, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)\n",
        "\n",
        "model = Classifier(len(marker2idx), 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n",
        "\n",
        "# (y > 2.5).float()\n",
        "# we convert y into boolean values\n",
        "# this is required to support the formulation we are considering in this section\n",
        "# (if it's greater than 2.5, then we are dealing with a positive review, otherwise it's negative)\n",
        "# BCE expects a probability as its gold, which is mathematically represented by a float, hence the .float operation\n",
        "training_loop(model, optimizer, adapt=lambda x, y : (x, (y > 2.5).float()))\n",
        "\n",
        "def predict(review: str):\n",
        "\n",
        "    # we need to vectorize the review\n",
        "    x = review2vector(review)\n",
        "\n",
        "    # our nn expects a batched input\n",
        "    # even if we have a single sample, we have to mock this structure converting a (11)-shape vector into (1, 11)-shape one\n",
        "    batched_x = x.unsqueeze(0)\n",
        "\n",
        "    # actual forward\n",
        "    batch_out = model(batched_x)\n",
        "\n",
        "    # return\n",
        "    return batch_out['pred'][0].item()\n",
        "\n",
        "def predict_and_print(review: str, stars: int):\n",
        "    print(f'=' * 125)\n",
        "    print(f'# review: {review}')\n",
        "    print(f'# gold stars: {stars} -> {1 if stars > 2.5 else 0}')\n",
        "    score = predict(review)\n",
        "    print(f'# predicted score: {score} -> {round(score)}')\n",
        "    print(f'=' * 125)\n",
        "\n",
        "\n",
        "# 4 star review\n",
        "predict_and_print('very good product. excellent quality', 4)\n",
        "\n",
        "# 1 star review\n",
        "predict_and_print('It echoes so badly with my voice. Don\\'t waste your time. I\\'m heavily considering returning it.', 1)\n",
        "\n",
        "n = 0\n",
        "d = 0\n",
        "\n",
        "for x, y in test_dataloader:\n",
        "\n",
        "    y = y > 2.5\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        batch_out = model(x)\n",
        "        pred = batch_out['pred']\n",
        "\n",
        "    pred = torch.round(pred)\n",
        "    d += pred.shape[0]\n",
        "    n += (y == pred).int().sum()\n",
        "\n",
        "print(f'# accuracy: {(n / d).item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wow!\n",
        "\n",
        "# Categorical Cross-Entropy\n",
        "\n",
        "**Problem**: given a review $r$, determine the number of stars $y \\in C = \\{1, 2, 3, 4, 5\\}$ it was originally assigned.\n",
        "\n",
        "While it may seem quite similar to the regressive formulation we did with MSE, this formulation actually involves a **classification task**; in particular, as $|C| > 2$, this is a **multi-class classification task** (or multi-label classification task). Given an input, starting from a set $C$ of output values, we should the determine the correct one.\n",
        "\n",
        "BCE here cannot be applied as it assumes that only $2$ values are being considered. Rather, its generalization to multi-class classification, **Categorical Cross-Entropy Loss** (often simply called Cross-Entropy Loss), is what is used:\n",
        "\n",
        "\\\\[ \\mathcal{L}_{CCE} = - \\frac{1}{|y|} \\sum_{i=1}^{|y|} \\sum_{j=1}^{|C|} y_{i,j} log(p(y_{i, j})) \\\\]\n",
        "\n",
        "Similarly to BCE, Cross-Entropy also works with probabilities. However, differently from BCE, rather than a single $p(y_i)$, Cross-Entropy expects a distribution over $C$, that is, a list of probabilities, one for each class $c$, that sums up to $1$. Visually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [0.2, 0.1, 0.05, 0.15, 0.5]\n",
        "assert sum(y) == 1.0\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.title('Distribution over C')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a bit more complex than what BCE requires and, most importantly, how can we make the NN emit it? Once more, it is quite straightforward. \n",
        "\n",
        "First, while up to this point the output shape of the final layer has always been $1$, we now change it to $|C|$; we can interpret the $j$-th value, which we denote with $z_j$, as the **score** the NN gives to class $C_j$. Then, we can convert these values into a probability distribution with a new activation function, the **softmax**:\n",
        "\n",
        "\\\\[ Softmax(z_{i, j}) = \\frac{e^{z_{i, j}}}{ \\sum_{j'=1}^{|y|} e^{z_{i, j'}} } \\\\]\n",
        "\n",
        "Thus:\n",
        "\n",
        "\\\\[ p(y_{i, j}) = Softmax(z_{i, j}) \\\\]\n",
        "\n",
        "The scores $z_0, \\dots, z_{|C|}$ are often referred to as the **logits** of the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea318ba5f90a44b59d253aefe0d9608d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d29a00007942c497d83c60ef6c4270",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a23b6988fab4ce5a0db11440adbe008",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b82620ae4a34726b2c0d6887291cb11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35ef3c23b562412ba9c05726979e7310",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=============================================================================================================================\n",
            "# review: very good product. excellent quality\n",
            "# gold stars: 4\n",
            "# predicted score: 5 -> 5\n",
            "=============================================================================================================================\n",
            "=============================================================================================================================\n",
            "# review: It echoes so badly with my voice. Don't waste your time. I'm heavily considering returning it.\n",
            "# gold stars: 1\n",
            "# predicted score: 1 -> 1\n",
            "=============================================================================================================================\n",
            "# accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_features: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(n_features, n_hidden)\n",
        "        # we have to emit logits over the values of C\n",
        "        # As |C|=5, the output dimension of the last layer should be $5$ as well\n",
        "        self.lin2 = torch.nn.Linear(n_hidden, 5)\n",
        "        # this time, CrossEntropyLoss\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "        # actual forward\n",
        "        out = self.lin1(x)\n",
        "        out = torch.relu(out)\n",
        "        out = self.lin2(out).squeeze(1)\n",
        "\n",
        "        # compute logits (which are simply the out variable) and the actual probability distribution (pred, as it is the predicted distribution)\n",
        "        logits = out\n",
        "        pred = torch.softmax(out, dim=-1)\n",
        "\n",
        "        result = {'logits': logits, 'pred': pred}\n",
        "\n",
        "        # compute loss\n",
        "        if y is not None:\n",
        "            # while mathematically the CrossEntropyLoss takes as input the probability distributions,\n",
        "            # torch optimizes its computation internally and takes as input the logits instead\n",
        "            loss = self.loss(logits, y)\n",
        "            result['loss'] = loss\n",
        "\n",
        "        return result\n",
        "\n",
        "    def loss(self, pred, y):\n",
        "        return self.loss_fn(pred, y)\n",
        "\n",
        "model = Classifier(len(marker2idx), 10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.0)\n",
        "\n",
        "# (y - 1).long()\n",
        "# from an implementation point of view, PyTorch Softmax expects a list of classes that start 0\n",
        "# as our classes start instead at 1, we need to offset them by -1\n",
        "# Softmax also expects a LongTensor, hence the .long() operation\n",
        "training_loop(model, optimizer, adapt=lambda x, y : (x, (y - 1).long()))\n",
        "\n",
        "def predict(review: str):\n",
        "\n",
        "    # we need to vectorize the review\n",
        "    x = review2vector(review)\n",
        "\n",
        "    # our nn expects a batched input\n",
        "    # even if we have a single sample, we have to mock this structure converting a (11)-shape vector into (1, 11)-shape one\n",
        "    batched_x = x.unsqueeze(0)\n",
        "\n",
        "    # actual forward\n",
        "    batch_out = model(batched_x)\n",
        "\n",
        "    # here it gets a bit tricky, let's extract the predicted probability distribution\n",
        "    pred = batch_out['pred']\n",
        "\n",
        "    # first of all, since we are mocking the batching behavior, let's just take the first\n",
        "    pred = pred[0]\n",
        "\n",
        "    # now, pred stores a probability distribution, assigning a probability p to each possible number of stars\n",
        "    # in order to extract the \"actual prediction\", we can simply select the one with the highest probability\n",
        "    pred = pred.argmax()\n",
        "\n",
        "    # let's sum 1 as usual and, finally, return\n",
        "    return pred.item() + 1\n",
        "\n",
        "def predict_and_print(review: str, stars: int):\n",
        "    print(f'=' * 125)\n",
        "    print(f'# review: {review}')\n",
        "    print(f'# gold stars: {stars}')\n",
        "    score = predict(review)\n",
        "    print(f'# predicted score: {score} -> {round(score)}')\n",
        "    print(f'=' * 125)\n",
        "\n",
        "\n",
        "# 4 star review\n",
        "predict_and_print('very good product. excellent quality', 4)\n",
        "\n",
        "# 1 star review\n",
        "predict_and_print('It echoes so badly with my voice. Don\\'t waste your time. I\\'m heavily considering returning it.', 1)\n",
        "\n",
        "n = 0\n",
        "d = 0\n",
        "\n",
        "for x, y in test_dataloader:\n",
        "\n",
        "    y = y - 1\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        batch_out = model(x)\n",
        "        pred = batch_out['pred']\n",
        "\n",
        "    pred = pred.argmax(-1)\n",
        "    d += pred.shape[0]\n",
        "    n += (y == pred).int().sum()\n",
        "\n",
        "print(f'# accuracy: {(n / d).item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It works once more!\n",
        "\n",
        "# Wrap Up and General Rules of Thumb\n",
        "\n",
        "* NN expects numerical input $\\rightarrow$ convert your data into numbers through some function $\\phi$ ($\\phi$ might be the identity function when dealing with some simple cases such as approximating the XOR function)\n",
        "* Regression Task $\\rightarrow$ use MSELoss\n",
        "* Binary Classification Task $\\rightarrow$ put a Sigmoid after your final Linear layer and use BCELoss\n",
        "* Multi-Class Classification Task $\\rightarrow$ set the output shape of your final layer to $|C|$, put a Softmax after it and use CrossEntropyLoss\n",
        "\n",
        "# Homeworks\n",
        "\n",
        "1. Try changing the considered markers and the function $\\phi$\n",
        "2. Choose one of the $3$ variants and try to re-implement everything with a slightly different architecture (use $3$ layers rather than $2$ for instance)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
